\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text.
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}

\pdfinfo{
   /Author (Anonymous)
   /Title  (Extended Abstract: Grounding Language to Reward Functions with Abstract Markov Decision Processes)
   /CreationDate (D:20160503120000)
   /Subject (AMDPs, Robots, Language)
   /Keywords (Grounded Language; AMDP)
}

\begin{document}

% paper title
\title{Extended Abstract: Grounding Language to Reward Functions With Abstract Markov Decision Processes}

% You will get a Paper-ID when submitting a pdf file to the conference system
\author{Author Names Omitted for Anonymous Review. Paper-ID [add your ID here]}

%\author{\authorblockN{Michael Shell}
%\authorblockA{School of Electrical and\\Computer Engineering\\
%Georgia Institute of Technology\\
%Atlanta, Georgia 30332--0250\\
%Email: mshell@ece.gatech.edu}
%\and
%\authorblockN{Homer Simpson}
%\authorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\authorblockN{James Kirk\\ and Montgomery Scott}
%\authorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}


% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership


% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
%
%\author{\authorblockN{Michael Shell\authorrefmark{1},
%Homer Simpson\authorrefmark{2},
%James Kirk\authorrefmark{3},
%Montgomery Scott\authorrefmark{3} and
%Eldon Tyrell\authorrefmark{4}}
%\authorblockA{\authorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: mshell@ece.gatech.edu}
%\authorblockA{\authorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\authorblockA{\authorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\authorblockA{\authorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}


\maketitle

\begin{abstract}
The abstract goes here.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
The issue of abstraction is key to the field of reinforcement learning as it represents a logical way to accelerate planning time within sequential decision making problems by breaking apart large, complex tasks into smaller sub-tasks. Most often, methods of abstraction have been applied to only a single element of the standard Markov decision process (MDP) formalism for reinforcement learning. Specifically, there have been some approaches that attempt to perform state abstraction~\cite{Li2006TowardsAU} by compressing similar states into single units or simply pruning irrelevant states from consideration. Other approaches examine abstraction over the action space via macro-actions~\cite{Hauskrecht1998HierarchicalSO} or options~\cite{Sutton1999BetweenMA} that compose sequences of atomic actions into a single ``action''. Unfortunately, many of these methods fall short in that they simply add more elements to either the state or action space thereby increasing the time needed for planning~\cite{Jong2008TheUO}.

Our approach in tackling this problem of abstraction is to utilize abstract Markov decision processes (AMDPs) which represent a decomposition of a regular MDPs creating multiple layers of abstraction. To briefly summarize, MDPs are represented by the five-tuple: $\langle$ ${\cal S}$, ${\cal A}$, ${\cal T}$, ${\cal R}$, $\gamma$ $\rangle$, where ${\cal S}$ is a set of states of the world; ${\cal A}$ is a set of actions that the agent can take; ${\cal T}$ is a function that computes the probability of transitioning from one state to another via applying a specified action; ${\cal R}$ is a function specifying the agent's reward for taking an action within a particular state ; and $\gamma$ is a discount factor. AMDPs take on a slightly different form by maintaining a stack of internal MDPs, each of which represents a different layer of abstraction for the input problem. The base level MDP represents the original task as it was given, while higher level MDPs are hard coded over abstracted state and action spaces (with corresponding transition dynamics and reward functions). These higher level MDPs capture the notion of sub-goals within the problem and so whenever the agent in a lower level need take an action, the current state information is fed upward to the next highest level of the AMDP. Once the highest MDP is reached, standard reinforcement learning algorithms are applied in order to compute a stationary policy for the entire level which is then stored and reused so as to avoid redundant computation. Surjective state mappings are maintained from higher to lower level MDPs allowing for a backpropagation to occur once a higher level sub-task has been solved and this process then repeats onnce the backpropagation returns all the way down to the base level MDP. It is important to note that since policies computed at each level of the AMDP are stored, the true cost of planning in an AMDP is only equivalent to the cost of planning and solving all higher level MDPs.

In order to examine the effectiveness of the abstraction imposed by AMDPs, we extend prior work by MacGlashan et.al. on grounding natural language commands to the reward function of an MDP~\cite{MacGlashan2015GroundingEC}. In particular, we take an AMDP and evaluate the effectiveness of grounding a reward function at each level of the AMDP using natural language commands taken individually from each level of abstraction. In the context of the cleanup world domain used by MacGlashan et.al., this corresponds to an AMDP with three layers of abstraction: high, mid, and low.

We take a natural language command from an arbitrary level and apply IBM Model II translation to ground it to a reward function for each level of the AMDP.

We show that grounding an arbitrary natural language command to a reward function will be most successful when the natural language command and MDP correspond to the same level of abstraction. For the purpose of staying consistent with the experimental method used by MacGlashan et.al., we will leverage objected-oriented MDPs~\cite{Diuk2008AnOR} within BURLAP to conduct experiments and collect results.

\section{Section}

Section text here.

\subsection{Subsection Heading Here}
Subsection text here.

\subsubsection{Subsubsection Heading Here}
Subsubsection text here.


\section{RSS citations}

Please make sure to include \verb!natbib.sty! and to use the
\verb!plainnat.bst! bibliography style. \verb!natbib! provides additional
citation commands, most usefully \verb!\citet!. For example, rather than the
awkward construction

{\small
\begin{verbatim}
\cite{kalman1960new} demonstrated...
\end{verbatim}
}

\noindent
rendered as ``\cite{kalman1960new} demonstrated...,''
or the
inconvenient

{\small
\begin{verbatim}
Kalman \cite{kalman1960new}
demonstrated...
\end{verbatim}
}

\noindent
rendered as
``Kalman \cite{kalman1960new} demonstrated...'',
one can
write

{\small
\begin{verbatim}
\citet{kalman1960new} demonstrated...
\end{verbatim}
}
\noindent
which renders as ``\citet{kalman1960new} demonstrated...'' and is
both easy to write and much easier to read.

\subsection{RSS Hyperlinks}

This year, we would like to use the ability of PDF viewers to interpret
hyperlinks, specifically to allow each reference in the bibliography to be a
link to an online version of the reference.
As an example, if you were to cite ``Passive Dynamic Walking''
\cite{McGeer01041990}, the entry in the bibtex would read:

{\small
\begin{verbatim}
@article{McGeer01041990,
  author = {McGeer, Tad},
  title = {\href{http://ijr.sagepub.com/content/9/2/62.abstract}{Passive Dynamic Walking}},
  volume = {9},
  number = {2},
  pages = {62-82},
  year = {1990},
  doi = {10.1177/027836499000900206},
  URL = {http://ijr.sagepub.com/content/9/2/62.abstract},
  eprint = {http://ijr.sagepub.com/content/9/2/62.full.pdf+html},
  journal = {The International Journal of Robotics Research}
}
\end{verbatim}
}
\noindent
and the entry in the compiled PDF would look like:

\def\tmplabel#1{[#1]}

\begin{enumerate}
\item[\tmplabel{1}] Tad McGeer. \href{http://ijr.sagepub.com/content/9/2/62.abstract}{Passive Dynamic
Walking}. {\em The International Journal of Robotics Research}, 9(2):62--82,
1990.
\end{enumerate}
%
where the title of the article is a link that takes you to the article on IJRR's website.


Linking cited articles will not always be possible, especially for
older articles. There are also often several versions of papers
online: authors are free to decide what to use as the link destination
yet we strongly encourage to link to archival or publisher sites
(such as IEEE Xplore or Sage Journals).  We encourage all authors to use this feature to
the extent possible.

\section{Conclusion}
\label{sec:conclusion}

The conclusion goes here.

\section*{Acknowledgments}

%% Use plainnat to work nicely with natbib.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}


