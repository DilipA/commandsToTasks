\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text.
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}

\pdfinfo{
   /Author (Anonymous)
   /Title  (Extended Abstract: Grounding Language to Reward Functions with Abstract Markov Decision Processes)
   /CreationDate (D:20160503120000)
   /Subject (AMDPs, Robots, Language)
   /Keywords (Grounded Language; AMDP)
}

\begin{document}

% paper title
\title{Extended Abstract: Grounding Language to Reward Functions With Abstract Markov Decision Processes}

% You will get a Paper-ID when submitting a pdf file to the conference system
\author{Author Names Omitted for Anonymous Review. Paper-ID [add your ID here]}

%\author{\authorblockN{Michael Shell}
%\authorblockA{School of Electrical and\\Computer Engineering\\
%Georgia Institute of Technology\\
%Atlanta, Georgia 30332--0250\\
%Email: mshell@ece.gatech.edu}
%\and
%\authorblockN{Homer Simpson}
%\authorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\authorblockN{James Kirk\\ and Montgomery Scott}
%\authorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}


% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership


% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
%
%\author{\authorblockN{Michael Shell\authorrefmark{1},
%Homer Simpson\authorrefmark{2},
%James Kirk\authorrefmark{3},
%Montgomery Scott\authorrefmark{3} and
%Eldon Tyrell\authorrefmark{4}}
%\authorblockA{\authorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: mshell@ece.gatech.edu}
%\authorblockA{\authorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\authorblockA{\authorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\authorblockA{\authorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}


\maketitle

\begin{abstract}
The abstract goes here.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
When dealing with the issue of efficiently solving large-scale sequential decision making problems defined by complex state-action spaces, the ability to identify and leverage multiple layers of abstraction is key. Developing good methods of abstraction represents a logical way to accelerate planning time by breaking apart large, complex tasks into smaller sub-tasks. Moreover, this notion of decomposing tasks into their smaller constituent pieces more closely resembles our own human methodology for solving problems by identifying and achieving sub-goals. Our goal in this work is to analyze an alternative method of abstraction for sequential decision making problems that utilize multiple layers of abstraction over all components of the Markov Decision Process (MPD) in order to efficiently learn in complex domains. In order to convey the strength of the abstraction imposed by the abstract Markov decision process (AMDP) formalism, we extend prior work on grounding natural language commands to reward functions and, in particular, evaluate the abstraction based on the success of these groundings.

\section{Prior Work}
Typically, methods of abstraction have been applied to only a single element of the standard Markov decision process (MDP) formalism in the hopes of improving overall problem tractability. Specifically, there have been some approaches that attempt to perform state abstraction~\cite{Li2006TowardsAU} by compressing similar states into single units or intelligently pruning states that are irrelevant to the overall task from consideration. Other approaches examine abstraction over the action space via macro-actions~\cite{Hauskrecht1998HierarchicalSO} or options~\cite{Sutton1999BetweenMA} that compose sequences of atomic actions into a single ``action''. Unfortunately, these methods suffer from potentially increasing the size of the state or action space thereby increasing the time needed for planning~\cite{Jong2008TheUO}.

\section{Abstract Markov Decision Processes}

Our approach in tackling this problem of abstraction is to utilize abstract Markov decision processes (AMDPs) which represent a decomposition of a regular MDP resulting in a tiered hierarchy where each level represents an abstraction over the previous one. To briefly summarize, MDPs are represented by a five-tuple: $\langle$ ${\cal S}$, ${\cal A}$, ${\cal T}$, ${\cal R}$, $\gamma$ $\rangle$, where ${\cal S}$ is the set of possible states in the world; ${\cal A}$ is a set of all available actions that can be executed; ${\cal T}$ is a function that computes the probability of transitioning from one state to another by executing an action; ${\cal R}$ is a function specifying the agent's reward for taking an action within a particular state ; and $\gamma$ is a discount factor.

Building on top of this, AMDPs take a layered approach by maintaining a stack of MDPs, each of which represents a different layer of abstraction for the input problem. The base (lowest) level MDP is always the MDP representing the original problem while higher level MDPs are hard coded over smaller state and action spaces (with corresponding transition dynamics and reward functions). Each state and action in a higher level MDP is implemented as an abstraction over multiple states and actions in the lower level creating a many-to-one mapping (as we move up the stack) resulting in smaller, more tractable state-action spaces. Intuitively, moving up the stack of MDPs captures the notion of identifying smaller, easier sub-goals within the problem. In terms of execution, a learning agent always begins at the base level MDP; whenever an action must be selected at a lower level, the current state information is fed upward and mapped to the next highest level of the AMDP effectively forcing the agent to think at one higher level of abstraction. This process repeats until the top of the MDP stack is reached, at which point, standard reinforcement learning algorithms are applied in order to compute a stationary policy for the entire level (which is then stored and reused so as to avoid redundant computation). Given this policy, the action required at highest level of abstraction becomes known and a series of surjective state mappings are maintained from higher to lower level MDPs allowing for a sort of backpropagation to occur once a higher level sub-task has been solved. This process of mapping backwards, computing a policy, and taking the specified action then repeats all the way down back to the base level MDP. It is important to note that since policies computed at each level of the AMDP are stored, the true cost of planning in an AMDP is only equivalent to the cost of planning and solving all higher level MDPs each of which is decreasing in size and complexity as we move up the AMDP stack.

\section{Evaluation}

\subsection{Experiments}

In order to examine the effectiveness of the abstractions imposed by these Abstract Markov Decision Processes (AMDPs), we extend prior work by MacGlashan et. al. on grounding natural language commands to the reward function of a Markov Decision Process (MDP). The primary contribution of this prior work is the ability to represent tasks as MDP reward functions and further combine learning from demonstration methods with language models to directly embed a user's verbal command into the reward function~\cite{MacGlashan2015GroundingEC}. Specifically, given a dataset of natural language commands and their associated task demonstrations, inverse reinforcement learning (IRL) is applied to the demonstrations in order to inform a distribution over possible reward functions which is then used to train a language model in a weakly supervised fashion. The authors followed the paradigm set up in MacGlashan et. al. and used an IBM Model II Translation System for translating between natural language and reward functions (machine language), augmented by the provided demonstration. More specifically, we modify the IBM Model II decoding procedure to weight the probability of a given translation (the probability of a machine language command given the source language command) by the probability of the machine language command given the provided demonstration. With this process, the success of the resultant models in grounding reward function tasks is dependent on both the various natural language commands across the dataset as well as the provided demonstrations.

We show that grounding an arbitrary natural language command to a reward function will be most successful when the natural language command and reward function correspond to the same level of abstraction. Staying consistent with the experimental method used by MacGlashan et.al., we leverage objected-oriented MDPs~\cite{Diuk2008AnOR} within the Brown-UMBC Reinforcement Learning and Planning library (BURLAP) to conduct our experiments. As with MacGlashan et. al. we use the Cleanup World Domain inspired by Sokoban \cite{Junghanns2001SokobanEG}.
% INSERT FIGURE

We build our own AMDP over this domain, with each level corresponding to a different level of abstraction. There are three levels: \textbf{low}, \textbf{mid}, and \textbf{high}. The low-level MDP corresponds to simple directional commands, like telling the agent to move north, south, east, and west. The mid-level MDP corresponds to commands referencing the agent, the block, the doors in each of the domain's three rooms, and the rooms themselves. Finally, the high-level MDP abstracts away doors, so that these commands only reference agents, blocks, and rooms.

To test the effectiveness of using AMDPs, we collect datasets for each level, with each dataset consisting of a list of natural language commands and their respective demonstrations. The following are some examples of natural language commands at each level of the AMDP:

\begin{description}
\item[\textbf{Low:}] Go three steps south, then two steps east.
\item[\textbf{Mid:}] Go to the red door, then go into the red room.
\item[\textbf{High:}] Go to the green room.
\end{description}

Our experiment consists of training IBM II Translation Models between every permutation of the levels of the AMDP. More precisely, we train a model to translate a natural language command to a machine language reward function for each level of the AMDP. We then
\subsection{Results}

Lorem ipsum

\section{Conclusion}
\label{sec:conclusion}

Lorem ipsum

\section*{Acknowledgments}

%% Use plainnat to work nicely with natbib.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}


